{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"세계 배달 피자 리더 도미노피자가 우리 고구마를 활용한 신메뉴를 출시한다.도미노피자는 오는 2월 1일 국내산 고구마와 4가지 치즈가 어우러진 신메뉴 `우리 고구마 피자`를 출시하고 전 매장에서 판매를 시작한다. 이번에 도미노피자가 내놓은 신메뉴 `우리 고구마 피자`는 까다롭게 엄선한 국내산 고구마를 무스와 큐브 형태로 듬뿍 올리고, 모차렐라, 카망베르, 체더 치즈와 리코타 치즈 소스 등 4가지 치즈와 와규 크럼블을 더한 프리미엄 고구마 피자다.\",\n",
    "    \"피자의 발상지이자 원조라고 할 수 있는 남부의 나폴리식 피자(Pizza Napolitana)는 재료 본연의 맛에 집중하여 뛰어난 식감을 자랑한다. 대표적인 나폴리 피자로는 피자 마리나라(Pizza Marinara)와 피자 마르게리타(Pizza Margherita)가 있다.\",\n",
    "    \"도미노피자가 삼일절을 맞아 '방문포장 1+1' 이벤트를 진행한다. 이번 이벤트는 도미노피자 102개 매장에서 3월 1일 단 하루 동안 방문포장 온라인, 오프라인 주문 시 피자 1판을 더 증정하는 이벤트다. 온라인 주문 시 장바구니에 2판을 담은 후 할인 적용이 가능하며, 동일 가격 또는 낮은 가격의 피자를 고객이 선택하면 무료로 증정한다.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sent):\n",
    "  return sent.split(\" \")\n",
    "\n",
    "tokenized_corpus = [tokenizer(doc) for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 27, 47]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'세계': 1,\n",
       "  '배달': 1,\n",
       "  '피자': 1,\n",
       "  '리더': 1,\n",
       "  '도미노피자가': 2,\n",
       "  '우리': 1,\n",
       "  '고구마를': 2,\n",
       "  '활용한': 1,\n",
       "  '신메뉴를': 1,\n",
       "  '출시한다.도미노피자는': 1,\n",
       "  '오는': 1,\n",
       "  '2월': 1,\n",
       "  '1일': 1,\n",
       "  '국내산': 2,\n",
       "  '고구마와': 1,\n",
       "  '4가지': 2,\n",
       "  '치즈가': 1,\n",
       "  '어우러진': 1,\n",
       "  '신메뉴': 2,\n",
       "  '`우리': 2,\n",
       "  '고구마': 3,\n",
       "  '피자`를': 1,\n",
       "  '출시하고': 1,\n",
       "  '전': 1,\n",
       "  '매장에서': 1,\n",
       "  '판매를': 1,\n",
       "  '시작한다.': 1,\n",
       "  '이번에': 1,\n",
       "  '내놓은': 1,\n",
       "  '피자`는': 1,\n",
       "  '까다롭게': 1,\n",
       "  '엄선한': 1,\n",
       "  '무스와': 1,\n",
       "  '큐브': 1,\n",
       "  '형태로': 1,\n",
       "  '듬뿍': 1,\n",
       "  '올리고,': 1,\n",
       "  '모차렐라,': 1,\n",
       "  '카망베르,': 1,\n",
       "  '체더': 1,\n",
       "  '치즈와': 2,\n",
       "  '리코타': 1,\n",
       "  '치즈': 1,\n",
       "  '소스': 1,\n",
       "  '등': 1,\n",
       "  '와규': 1,\n",
       "  '크럼블을': 1,\n",
       "  '더한': 1,\n",
       "  '프리미엄': 1,\n",
       "  '피자다.': 1},\n",
       " {'피자의': 1,\n",
       "  '발상지이자': 1,\n",
       "  '원조라고': 1,\n",
       "  '할': 1,\n",
       "  '수': 1,\n",
       "  '있는': 1,\n",
       "  '남부의': 1,\n",
       "  '나폴리식': 1,\n",
       "  '피자(Pizza': 1,\n",
       "  'Napolitana)는': 1,\n",
       "  '재료': 1,\n",
       "  '본연의': 1,\n",
       "  '맛에': 1,\n",
       "  '집중하여': 1,\n",
       "  '뛰어난': 1,\n",
       "  '식감을': 1,\n",
       "  '자랑한다.': 1,\n",
       "  '대표적인': 1,\n",
       "  '나폴리': 1,\n",
       "  '피자로는': 1,\n",
       "  '피자': 2,\n",
       "  '마리나라(Pizza': 1,\n",
       "  'Marinara)와': 1,\n",
       "  '마르게리타(Pizza': 1,\n",
       "  'Margherita)가': 1,\n",
       "  '있다.': 1},\n",
       " {'도미노피자가': 1,\n",
       "  '삼일절을': 1,\n",
       "  '맞아': 1,\n",
       "  \"'방문포장\": 1,\n",
       "  \"1+1'\": 1,\n",
       "  '이벤트를': 1,\n",
       "  '진행한다.': 1,\n",
       "  '이번': 1,\n",
       "  '이벤트는': 1,\n",
       "  '도미노피자': 1,\n",
       "  '102개': 1,\n",
       "  '매장에서': 1,\n",
       "  '3월': 1,\n",
       "  '1일': 1,\n",
       "  '단': 1,\n",
       "  '하루': 1,\n",
       "  '동안': 1,\n",
       "  '방문포장': 1,\n",
       "  '온라인,': 1,\n",
       "  '오프라인': 1,\n",
       "  '주문': 2,\n",
       "  '시': 2,\n",
       "  '피자': 1,\n",
       "  '1판을': 1,\n",
       "  '더': 1,\n",
       "  '증정하는': 1,\n",
       "  '이벤트다.': 1,\n",
       "  '온라인': 1,\n",
       "  '장바구니에': 1,\n",
       "  '2판을': 1,\n",
       "  '담은': 1,\n",
       "  '후': 1,\n",
       "  '할인': 1,\n",
       "  '적용이': 1,\n",
       "  '가능하며,': 1,\n",
       "  '동일': 1,\n",
       "  '가격': 1,\n",
       "  '또는': 1,\n",
       "  '낮은': 1,\n",
       "  '가격의': 1,\n",
       "  '피자를': 1,\n",
       "  '고객이': 1,\n",
       "  '선택하면': 1,\n",
       "  '무료로': 1,\n",
       "  '증정한다.': 1}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'세계': 0.5108256237659907,\n",
       " '배달': 0.5108256237659907,\n",
       " '피자': 0.11580621302033972,\n",
       " '리더': 0.5108256237659907,\n",
       " '도미노피자가': 0.11580621302033972,\n",
       " '우리': 0.5108256237659907,\n",
       " '고구마를': 0.5108256237659907,\n",
       " '활용한': 0.5108256237659907,\n",
       " '신메뉴를': 0.5108256237659907,\n",
       " '출시한다.도미노피자는': 0.5108256237659907,\n",
       " '오는': 0.5108256237659907,\n",
       " '2월': 0.5108256237659907,\n",
       " '1일': 0.11580621302033972,\n",
       " '국내산': 0.5108256237659907,\n",
       " '고구마와': 0.5108256237659907,\n",
       " '4가지': 0.5108256237659907,\n",
       " '치즈가': 0.5108256237659907,\n",
       " '어우러진': 0.5108256237659907,\n",
       " '신메뉴': 0.5108256237659907,\n",
       " '`우리': 0.5108256237659907,\n",
       " '고구마': 0.5108256237659907,\n",
       " '피자`를': 0.5108256237659907,\n",
       " '출시하고': 0.5108256237659907,\n",
       " '전': 0.5108256237659907,\n",
       " '매장에서': 0.11580621302033972,\n",
       " '판매를': 0.5108256237659907,\n",
       " '시작한다.': 0.5108256237659907,\n",
       " '이번에': 0.5108256237659907,\n",
       " '내놓은': 0.5108256237659907,\n",
       " '피자`는': 0.5108256237659907,\n",
       " '까다롭게': 0.5108256237659907,\n",
       " '엄선한': 0.5108256237659907,\n",
       " '무스와': 0.5108256237659907,\n",
       " '큐브': 0.5108256237659907,\n",
       " '형태로': 0.5108256237659907,\n",
       " '듬뿍': 0.5108256237659907,\n",
       " '올리고,': 0.5108256237659907,\n",
       " '모차렐라,': 0.5108256237659907,\n",
       " '카망베르,': 0.5108256237659907,\n",
       " '체더': 0.5108256237659907,\n",
       " '치즈와': 0.5108256237659907,\n",
       " '리코타': 0.5108256237659907,\n",
       " '치즈': 0.5108256237659907,\n",
       " '소스': 0.5108256237659907,\n",
       " '등': 0.5108256237659907,\n",
       " '와규': 0.5108256237659907,\n",
       " '크럼블을': 0.5108256237659907,\n",
       " '더한': 0.5108256237659907,\n",
       " '프리미엄': 0.5108256237659907,\n",
       " '피자다.': 0.5108256237659907,\n",
       " '피자의': 0.5108256237659907,\n",
       " '발상지이자': 0.5108256237659907,\n",
       " '원조라고': 0.5108256237659907,\n",
       " '할': 0.5108256237659907,\n",
       " '수': 0.5108256237659907,\n",
       " '있는': 0.5108256237659907,\n",
       " '남부의': 0.5108256237659907,\n",
       " '나폴리식': 0.5108256237659907,\n",
       " '피자(Pizza': 0.5108256237659907,\n",
       " 'Napolitana)는': 0.5108256237659907,\n",
       " '재료': 0.5108256237659907,\n",
       " '본연의': 0.5108256237659907,\n",
       " '맛에': 0.5108256237659907,\n",
       " '집중하여': 0.5108256237659907,\n",
       " '뛰어난': 0.5108256237659907,\n",
       " '식감을': 0.5108256237659907,\n",
       " '자랑한다.': 0.5108256237659907,\n",
       " '대표적인': 0.5108256237659907,\n",
       " '나폴리': 0.5108256237659907,\n",
       " '피자로는': 0.5108256237659907,\n",
       " '마리나라(Pizza': 0.5108256237659907,\n",
       " 'Marinara)와': 0.5108256237659907,\n",
       " '마르게리타(Pizza': 0.5108256237659907,\n",
       " 'Margherita)가': 0.5108256237659907,\n",
       " '있다.': 0.5108256237659907,\n",
       " '삼일절을': 0.5108256237659907,\n",
       " '맞아': 0.5108256237659907,\n",
       " \"'방문포장\": 0.5108256237659907,\n",
       " \"1+1'\": 0.5108256237659907,\n",
       " '이벤트를': 0.5108256237659907,\n",
       " '진행한다.': 0.5108256237659907,\n",
       " '이번': 0.5108256237659907,\n",
       " '이벤트는': 0.5108256237659907,\n",
       " '도미노피자': 0.5108256237659907,\n",
       " '102개': 0.5108256237659907,\n",
       " '3월': 0.5108256237659907,\n",
       " '단': 0.5108256237659907,\n",
       " '하루': 0.5108256237659907,\n",
       " '동안': 0.5108256237659907,\n",
       " '방문포장': 0.5108256237659907,\n",
       " '온라인,': 0.5108256237659907,\n",
       " '오프라인': 0.5108256237659907,\n",
       " '주문': 0.5108256237659907,\n",
       " '시': 0.5108256237659907,\n",
       " '1판을': 0.5108256237659907,\n",
       " '더': 0.5108256237659907,\n",
       " '증정하는': 0.5108256237659907,\n",
       " '이벤트다.': 0.5108256237659907,\n",
       " '온라인': 0.5108256237659907,\n",
       " '장바구니에': 0.5108256237659907,\n",
       " '2판을': 0.5108256237659907,\n",
       " '담은': 0.5108256237659907,\n",
       " '후': 0.5108256237659907,\n",
       " '할인': 0.5108256237659907,\n",
       " '적용이': 0.5108256237659907,\n",
       " '가능하며,': 0.5108256237659907,\n",
       " '동일': 0.5108256237659907,\n",
       " '가격': 0.5108256237659907,\n",
       " '또는': 0.5108256237659907,\n",
       " '낮은': 0.5108256237659907,\n",
       " '가격의': 0.5108256237659907,\n",
       " '피자를': 0.5108256237659907,\n",
       " '고객이': 0.5108256237659907,\n",
       " '선택하면': 0.5108256237659907,\n",
       " '무료로': 0.5108256237659907,\n",
       " '증정한다.': 0.5108256237659907}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65960979, 0.        , 0.49736316])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"도미노피자 신메뉴\"\n",
    "token_query = tokenizer(query)\n",
    "\n",
    "bm25.get_scores(token_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['세계 배달 피자 리더 도미노피자가 우리 고구마를 활용한 신메뉴를 출시한다.도미노피자는 오는 2월 1일 국내산 고구마와 4가지 치즈가 어우러진 신메뉴 `우리 고구마 피자`를 출시하고 전 매장에서 판매를 시작한다. 이번에 도미노피자가 내놓은 신메뉴 `우리 고구마 피자`는 까다롭게 엄선한 국내산 고구마를 무스와 큐브 형태로 듬뿍 올리고, 모차렐라, 카망베르, 체더 치즈와 리코타 치즈 소스 등 4가지 치즈와 와규 크럼블을 더한 프리미엄 고구마 피자다.',\n",
       " \"도미노피자가 삼일절을 맞아 '방문포장 1+1' 이벤트를 진행한다. 이번 이벤트는 도미노피자 102개 매장에서 3월 1일 단 하루 동안 방문포장 온라인, 오프라인 주문 시 피자 1판을 더 증정하는 이벤트다. 온라인 주문 시 장바구니에 2판을 담은 후 할인 적용이 가능하며, 동일 가격 또는 낮은 가격의 피자를 고객이 선택하면 무료로 증정한다.\",\n",
       " '피자의 발상지이자 원조라고 할 수 있는 남부의 나폴리식 피자(Pizza Napolitana)는 재료 본연의 맛에 집중하여 뛰어난 식감을 자랑한다. 대표적인 나폴리 피자로는 피자 마리나라(Pizza Marinara)와 피자 마르게리타(Pizza Margherita)가 있다.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(token_query, corpus, n= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "open_path = '/opt/ml/github/Competition_MRC/data/wikipedia_documents.json'\n",
    "data_path = '/opt/ml/github/Competition_MRC/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(open_path), \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m contexts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m wiki\u001b[38;5;241m.\u001b[39mvalues()]))\n",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m contexts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys([\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m wiki\u001b[38;5;241m.\u001b[39mvalues()]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'context'"
     ]
    }
   ],
   "source": [
    "contexts.append(dict.fromkeys([v[\"context\"] for v in wiki.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60613"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56737"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wiki).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corpus_source</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>html</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>나라 목록</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>나라 목록</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현 서울특별시 종로구 서린동 (구 일제 강점기 경기도 경성부 서린정) 출신이다. 친...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>백남준</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아오조라 문고(靑空文庫, あおぞらぶんこ|아오조라 분고)는 ‘일본어판 구텐베르크 프로...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>아오조라 문고</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>저자 사망 이후 50년이 지나 저작권이 소멸한 메이지 시대부터 쇼와 시대 초기까지의...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>아오조라 문고</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60608</th>\n",
       "      <td>오키나와 현립 박물관·미술관( , Okinawa Prefectural Museum ...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>오키나와 현립 박물관·미술관</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>60608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60609</th>\n",
       "      <td>1936년 7월, 오키나와현 교육위원회 부설 향토 박물관(沖縄県教育会附設郷土博物館)...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>오키나와 현립 박물관·미술관</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>60609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60610</th>\n",
       "      <td>박물관은 개관 10주년이되는 2017년에 관의 애칭 및 마스코트를 일반인에게 공모했...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>오키나와 현립 박물관·미술관</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>60610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60611</th>\n",
       "      <td>단결권 및 단체교섭권 협약(Right to Organise and Collectiv...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>단결권 및 단체교섭권 협약</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>60611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60612</th>\n",
       "      <td>이 협약은 부당노동행위 제도를 규율하고 있다. 협약 제1조에서 반노동조합 차별행위로...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>단결권 및 단체교섭권 협약</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>60612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56737 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text corpus_source   url  \\\n",
       "0      이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개...         위키피디아  TODO   \n",
       "1      이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정...         위키피디아  TODO   \n",
       "2      현 서울특별시 종로구 서린동 (구 일제 강점기 경기도 경성부 서린정) 출신이다. 친...         위키피디아  TODO   \n",
       "3      아오조라 문고(靑空文庫, あおぞらぶんこ|아오조라 분고)는 ‘일본어판 구텐베르크 프로...         위키피디아  TODO   \n",
       "4      저자 사망 이후 50년이 지나 저작권이 소멸한 메이지 시대부터 쇼와 시대 초기까지의...         위키피디아  TODO   \n",
       "...                                                  ...           ...   ...   \n",
       "60608  오키나와 현립 박물관·미술관( , Okinawa Prefectural Museum ...         위키피디아  None   \n",
       "60609  1936년 7월, 오키나와현 교육위원회 부설 향토 박물관(沖縄県教育会附設郷土博物館)...         위키피디아  None   \n",
       "60610  박물관은 개관 10주년이되는 2017년에 관의 애칭 및 마스코트를 일반인에게 공모했...         위키피디아  None   \n",
       "60611  단결권 및 단체교섭권 협약(Right to Organise and Collectiv...         위키피디아  None   \n",
       "60612  이 협약은 부당노동행위 제도를 규율하고 있다. 협약 제1조에서 반노동조합 차별행위로...         위키피디아  None   \n",
       "\n",
       "      domain            title author  html document_id  \n",
       "0       None            나라 목록   None  None           0  \n",
       "1       None            나라 목록   None  None           1  \n",
       "2       None              백남준   None  None           2  \n",
       "3       None          아오조라 문고   None  None           3  \n",
       "4       None          아오조라 문고   None  None           4  \n",
       "...      ...              ...    ...   ...         ...  \n",
       "60608   None  오키나와 현립 박물관·미술관   None  None       60608  \n",
       "60609   None  오키나와 현립 박물관·미술관   None  None       60609  \n",
       "60610   None  오키나와 현립 박물관·미술관   None  None       60610  \n",
       "60611   None   단결권 및 단체교섭권 협약   None  None       60611  \n",
       "60612   None   단결권 및 단체교섭권 협약   None  None       60612  \n",
       "\n",
       "[56737 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = load_from_disk('/opt/ml/github/Competition_MRC/data/train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = org_data['train']\n",
    "val_data = org_data['validation']\n",
    "var = [train_data, val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in var:\n",
    "    contexts += x['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사회주의 혁명은 오로지 선진노동자계급에 기초한 계급투쟁으로서 이루어질 수 있다고 주장한 레닌의 노선은 본질적으로는 프랑스의 사회주의자인 오귀스트 블랑키(Auguste Blanqui)의 비밀결사주의와 동일하지 않다. 다음은 블랑키주의에 대한 레닌의 비판이다.\\\\n블랑키주의는 계급투쟁을 긍정하는 이론이다. 그러나 블랑키주의는 프롤레타리아의 계급투쟁에 의거하지 않고 소수 인텔리겐차의 음모로써 인류가 임금노예제로부터 해방될 것을 기대한 것이다. 블랑키의 행동 지침은 부르주아 민주주의의 계급 모순을 지각한 혁명적 부르주아의 일반적인 경향이며, 노동계급에 의한 계급의식의 표출과는 무관한 것이다.|블라디미르 레닌, 『대회의 총결과에 붙여서』(1906년) \\\\n동시에 레닌은 블랑키주의가 소수 지식인의 음모에 의한 혁명 방식이며, 소수에 의한 쿠데타와 다를 바가 없다고 비판하기도 하였다.\\\\n\\\\n그러나, 경제적 후진성과 러시아 정교회를 비롯한 여러 반동적 사상 조류가 극심했던 당시 러시아 사회의 특성을 고려하여, 소수 직업혁명가의 역량 확보를 강조하였고, 이 지점에서 블랑키의 사상과 밀접한 연관을 이루게 됐다. 특히 폭력혁명에 대한 긍정 및 합법 활동과 비합법 활동을 혁명의 성취라는 목적에 따라 적절히 배합해야 한다는 레닌의 주장은 블랑키의 주장과 상당히 흡사한 지점이다.\\\\n\\\\n특히, 당원의 지적 수련, 금욕적 생활, 사생취의(捨生取義) 정신을 강조했다는 점과, 일반적인 노동자계급과, 노동자계급을 지도하는 직업 혁명가의 뚜렷한 구분은 기존 마르크스주의자들과 달랐던 지점이다. 이러한 지점은 여러 학자들에 의해 블랑키주의의 영향을 받았다고 평가받는다. 공산주의 혁명가인 로자 룩셈부르크는 레닌과 블랑키의 차이는 지엽적이며, 본질적으로는 같은 것이라고 평가하였다. 이러한 비판은 러시아 10월 혁명이 “순수한 프롤레타리아 계급에 의한 혁명인가?”, 아니면 “소수 혁명적 지식인에 의한 쿠데타인가?”라는 본질적인 논쟁과 맞닿은 것이다.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56737"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "    num_rows: 3952\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(contexts, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['대통령을', '포함한', '미국의', '행정부', '견제권을', '갖는', '국가', '기관은?'], ['현대적', '인사조직관리의', '시발점이', '된', '책은?'], ['강희제가', '1717년에', '쓴', '글은', '누구를', '위해', '쓰여졌는가?']]\n"
     ]
    }
   ],
   "source": [
    "query = train_data['question'][:3]\n",
    "query_ex = [tokenizer(x) for x in query]\n",
    "print(query_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?',\n",
       " '현대적 인사조직관리의 시발점이 된 책은?',\n",
       " '강희제가 1717년에 쓴 글은 누구를 위해 쓰여졌는가?']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = []\n",
    "for q in query:\n",
    "    tmp = bm25.get_top_n(q, contexts, n=3)\n",
    "    tk.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\relative c\\' { \\\\key c \\\\major \\\\set Staff.midiInstrument = #\"violin\"\\nr2.(r8) g8 | c4 c8. d16 b4 b8. c16 | a2. r8 c8 | f4 e8. f16 d4 e8. f16 | e2. r8 c8 |\\\\break\\na\\'4 f8. e16 d4 e8. f16 | g4 e4 c4 c8. b16 | a4. f\\'8 e4. d8 | c2. r8 g8 |\\\\break\\nc4 c8. d16 b4 b8. c16 | a2. r8 c8 | f4 e8. f16 d8. d16 e8. f16 | e2. r8 c8 |\\\\break\\na\\'4 f8. e16 d8. d16 e8. f16 | g4 e4 c4 c8. b16 | a4. f\\'8 e4. d8 |\\\\break\\nc2 r4 b8. c16 | d4. b8 b8. g16 g8. f\\'16 | e2 r4 d8. e16 | f4. f8 f8. f16 g8. d16 |\\\\break\\ne4 g4 c4 c8. b16 | a8. a16 a16 b16 c8 c8. a16 a16 b16 c8 |\\\\break\\nc8. c16 c8. c16 c8. c16 c8. d16 | b2 r4 g16 g16 g8 | c4 c8. d16 b4 b8. c16 |\\\\break\\na2 r4 c16 c16 c8 | f4 e8. f16 d8. d16 e8. f16 | e2 r4 c16 c16 c8 |\\\\break\\na\\'4 f8. e16 d4 e8. f16 | g4 e4 c4 c8. b16 | a4. f\\'8 e4. d8 | c2. r4 \\\\bar\"|.\"}\\n\\n\\\\addlyrics {\\n우 린 그 렇 게 두 렵 다 우 린 그 토 록 애절 하 다\\n고 개 들 면 서 구 호 외 치면 서 자 유 는 다 시 오 길\\n총 알 눈 앞 에 지 나 가 연 기 목 안 에 머 무 른 다\\n피 가 흘 러 도 - 한 걸 음 씩 간다 우 리 의 자 유를 위 해\\n벌 벌 떨 었 던 - 그 날 들 옆 에 서 몰 랐 던 친 구 와 함 께\\n서 서 히 자 유 를 위 해 큰 힘 을 모 여 용 기 내 어 함 께 걸 어\\n다 시 오 른 홍 콩 의 태 양 아 정 의 구 현 위 하 여 시 대 혁 명\\n민 주 자 유 영 원 히 홍 콩 비 추 길 영 광 이 다 시 오 길\\n}',\n",
       " \"1 정 당 조달 가격은 약 200 만엔 (1985년 당시), 엔달러 환율이 250엔이었던 그 시대 달러 가치로 8,000달러였다. 2018년 현재 한화가치로는 1,200만원에 달하는 기관총이다. 프레스 가공 을 다용 한 것으로, 당시로서는 높은 생산성을 자랑했다. 총신 은 낮추고 손 (운반 손잡이)와 일체가되어 2.5 초에서 교환 가능하며, 강내는 내구성을 높이기위한 크롬 도금이되어있어 외주에는 총신의 과열을 줄이기 위해 냉각 핀을 갖춘다. 디자인도 64 식 7.62mm 소총 과 마찬가지로 일본인 의 체격을 고려한 것으로되어있다. 이들은 당시 미군 에서 운용되고 있던 M60 기관총 보다 우수한 점이다 여겨진다. 작동 방식은 가스압 이용 식 ( 롱 스트로크 피스톤 식 )이며,規整子(레귤레이터)에 의해 가스 유입량을 조정하여 발사 속도를 변경할 수있다.\\n\\n볼트 폐쇄기구는 틸트 볼트 식 의 일종 인 '전단 요동 식 틸팅 볼트 폐쇄기구'라는 특수한기구를 채용하고있다.\\n\\n수신기는 중량 경감을 위해 스틸 강판을 프레스 성형 가공하여 제작되었다. 단면이 사각형의 상자 형으로 리벳으로 조립되고있다.\\n\\n사용 탄약 은 7.62x51mm NATO 탄 을 사용한다. 일반적으로 발사 약물을 감소시킨 감소 장탄을 사용하지만 통상 탄 사용 가능으로되어있다. 급탄은 미군과 NATO 제식 금속 분리 식 M13 링크 에 의해 이루어진다.\\n\\n안전 장치는 방아쇠 부 우측 상단에 안전자를 앞으로 돌리면 안전 장치가 붙들把측에 반 회전 시키면 격발 위치가된다. 遊底포장은遊底덮개 래치를 총구 방향으로 누르면 열리지 만, 총알 의 장전은 덮개를 열지 않고도 가능하다.\\n\\n양각대가 달려 있어서 엎드려 누워서 사격이 가능하다.\",\n",
       " '\\\\relative c\\' { \\\\key g \\\\major \\\\time 12/4  \\\\tempo \"Lento\" 4 = 72 \\\\set Staff.midiInstrument = #\"violin\"\\nb8 b8 b2 e8 e~e4 e8(g) | b8 b8 a16(g16 b8) a8(g8) e16(d16 b8 ~ b4 e16 d16 b8) |\\\\break\\ne8 e8 e4 e8(g8) b8 b8 a8(g8) e16(d16 b8) | e4 e8(g8) a16(g16 e8) e4 ~ e4 r4 |\\\\break\\nd\\'8 b8 b2 b8 b8 b2 | b8 b8 a16(g16 b8) a8(g8) e16(d16 b8 ~ b4 e16 d16 b8) |\\\\break\\ne8 e8 e4 e8(g8)  g8 b8 a8(g8) e16(d16 b8) | e8 e8 e8(g8) a16(g16 e8) e4 ~ e4 r4 \\\\bar \"|.\"}\\n\\\\addlyrics {\\n아 무 렴 그 렇 지 그 렇 고 말 고 한 오 백 년 살 자 는 데 웬 성 화 요\\n한 많 은 이 세 상 야 속 한 임 아 정 을 두 고 몸 만 가 니 눈 물 이 나 네\\n}']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval - BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Features, Value, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from retrieval.get_embedding import get_sparse_embedding, build_faiss\n",
    "from retrieval.get_relevant_doc import get_relevant_doc, get_relevant_doc_faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Features, Value, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from retrieval.get_embedding import get_sparse_embedding, build_faiss\n",
    "from retrieval.get_relevant_doc import get_relevant_doc, get_relevant_doc_faiss\n",
    "\n",
    "class SparseRetrieval:\n",
    "    def __init__(self,\n",
    "                tokenizer,\n",
    "                data_path: Optional[str] = \"data/\",\n",
    "                context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "                is_faiss: bool = False):\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenizer:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.is_faiss = is_faiss\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        # 중복 데이터를 제거(set은 순서가 뒤바뀌므로 for loop을 통해 제거)\n",
    "        self.contexts = list({v[\"text\"]:None for v in wiki.values()})\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "\n",
    "        # Transform by vectorizer\n",
    "        # tfidfv = TfidfVectorizer(tokenizer=tokenizer.tokenize, ngram_range=(1, 2), max_features=50000)\n",
    "        bm25 = BM25Okapi(self.contexts,tokenizer=tokenizer)\n",
    "\n",
    "        # self.passage_embedding, self.tfidfv = get_sparse_embedding(data_path, tfidfv, self.contexts)\n",
    "        self.passage_embedding, self.bm25 = get_sparse_embedding(data_path, bm25, self.contexts)\n",
    "        # if self.is_faiss:\n",
    "        #     self.indexer = build_faiss(data_path, self.passage_embedding, num_clusters=64)\n",
    "\n",
    "    def retrieve(self,\n",
    "                query_or_dataset: Union[str, Dataset],\n",
    "                topk: Optional[int] = 1\n",
    "                ) -> Union[Tuple[List, List], DatasetDict]:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query_or_dataset (Union[str, Dataset]):\n",
    "                str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "                str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "                Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "                이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "            topk (Optional[int], optional): Defaults to 1.\n",
    "                상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "\n",
    "        Returns:\n",
    "            1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "            다수의 Query를 받는 경우 -> DatasetDict: [description]\n",
    "\n",
    "        Note:\n",
    "            다수의 Query를 받는 경우,\n",
    "                Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "                Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str): # bulk = False\n",
    "            if self.is_faiss:\n",
    "                doc_scores, doc_indices = get_relevant_doc_faiss(query_or_dataset, self.tfidfv, self.indexer,  k=topk, bulk=False)\n",
    "            elif not self.is_faiss:\n",
    "                doc_scores, doc_indices = get_relevant_doc(query_or_dataset, self.tfidfv, self.passage_embedding,  k=topk, bulk=False)\n",
    "\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "            for i in range(topk):\n",
    "                print(f\"Top-{i+1} passage with score {doc_scores[i]:4f}\")\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset): # bulk = True\n",
    "            if self.is_faiss:\n",
    "                doc_scores, doc_indices = get_relevant_doc_faiss(query_or_dataset['question'], self.tfidfv, self.indexer, k=topk, bulk=True)\n",
    "            elif not self.is_faiss:\n",
    "                doc_scores, doc_indices = get_relevant_doc(query_or_dataset['question'], self.tfidfv, self.passage_embedding, k=topk, bulk=True)\n",
    "\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            total = []\n",
    "            for idx, example in enumerate(tqdm(query_or_dataset, desc=\"Sparse retrieval: \")):\n",
    "                tmp = {\n",
    "                    \"question\": example[\"question\"], # query\n",
    "                    \"id\": example[\"id\"], # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context\": \" \".join([self.contexts[pid] for pid in doc_indices[idx]]),\n",
    "                }\n",
    "                # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            f = Features({\"context\": Value(dtype=\"string\", id=None),\n",
    "                          \"id\": Value(dtype=\"string\", id=None),\n",
    "                          \"question\": Value(dtype=\"string\", id=None),})\n",
    "            \n",
    "            datasets = DatasetDict({\"validation\": Dataset.from_pandas(pd.DataFrame(total), features=f)})\n",
    "            return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import AutoTokenizer\n",
    "# def get_sparse_embedding(data_path, tfidfv, contexts):\n",
    "def get_sparse_embedding(data_path, bm25, contexts):    \n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        Passage Embedding을 만들고\n",
    "        TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "        만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "    # Pickle을 저장합니다.\n",
    "    pickle_name = f\"sparse_embedding.bin\"\n",
    "    # tfidfv_name = f\"tfidv.bin\"\n",
    "    bm25_name = f\"bm25.bin\"\n",
    "    emd_path = os.path.join(data_path, pickle_name)\n",
    "    # tfidfv_path = os.path.join(data_path, tfidfv_name)\n",
    "    bm25_path = os.path.join(data_path,bm25_name)\n",
    "\n",
    "    # if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path):\n",
    "    if os.path.isfile(emd_path) and os.path.isfile(bm25_path):\n",
    "        with open(emd_path, \"rb\") as file:\n",
    "            passage_embedding = pickle.load(file)\n",
    "        with open(bm25_path, \"rb\") as file:\n",
    "        # with open(tfidfv_path, \"rb\") as file:\n",
    "            # tfidfv = pickle.load(file)\n",
    "            bm25 = pickle.load(file)\n",
    "        print(\"Embedding pickle load.\")\n",
    "    else:\n",
    "        print(\"Build passage embedding\")\n",
    "        # passage_embedding = tfidfv.fit_transform(contexts)\n",
    "        passage_embedding = BM25Okapi(contexts,tokenizer=tokenizer)\n",
    "        print('passage_embedding.shape', passage_embedding.shape)\n",
    "        with open(emd_path, \"wb\") as file:\n",
    "            pickle.dump(passage_embedding, file)\n",
    "        # with open(tfidfv_path, \"wb\") as file:\n",
    "        with open(bm25_path, \"wb\") as file:\n",
    "            # pickle.dump(tfidfv, file)\n",
    "            pickle.dump(bm25, file)\n",
    "        print(\"Embedding pickle saved.\")\n",
    "    \n",
    "    # return passage_embedding, tfidfv\n",
    "    return passage_embedding, bm25\n",
    "\n",
    "# def build_faiss(data_path, passage_embedding, num_clusters=64):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Summary:\n",
    "#         속성으로 저장되어 있는 Passage Embedding을\n",
    "#         Faiss indexer에 fitting 시켜놓습니다.\n",
    "#         이렇게 저장된 indexer는 `get_relevant_doc`에서 유사도를 계산하는데 사용됩니다.\n",
    "\n",
    "#     Note:\n",
    "#         Faiss는 Build하는데 시간이 오래 걸리기 때문에,\n",
    "#         매번 새롭게 build하는 것은 비효율적입니다.\n",
    "#         그렇기 때문에 build된 index 파일을 저정하고 다음에 사용할 때 불러옵니다.\n",
    "#         다만 이 index 파일은 용량이 1.4Gb+ 이기 때문에 여러 num_clusters로 시험해보고\n",
    "#         제일 적절한 것을 제외하고 모두 삭제하는 것을 권장합니다.\n",
    "#     \"\"\"\n",
    "\n",
    "#     indexer_name = f\"faiss_clusters{num_clusters}.index\"\n",
    "#     indexer_path = os.path.join(data_path, indexer_name)\n",
    "#     if os.path.isfile(indexer_path):\n",
    "#         print(\"Load Saved Faiss Indexer.\")\n",
    "#         indexer = faiss.read_index(indexer_path)\n",
    "\n",
    "#     else:\n",
    "#         p_emb = passage_embedding.astype(np.float32).toarray()\n",
    "#         emb_dim = p_emb.shape[-1]\n",
    "\n",
    "#         num_clusters = num_clusters\n",
    "#         quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "\n",
    "#         indexer = faiss.IndexIVFScalarQuantizer(\n",
    "#             quantizer, quantizer.d, num_clusters, faiss.METRIC_L2\n",
    "#         )\n",
    "#         indexer.train(p_emb)\n",
    "#         indexer.add(p_emb)\n",
    "#         faiss.write_index(indexer, indexer_path)\n",
    "#         print(\"Faiss Indexer Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def get_relevant_doc(\n",
    "    query: str,\n",
    "    # tfidfv,\n",
    "    bm25,\n",
    "    p_embedding,\n",
    "    k: Optional[int] = 1, \n",
    "    bulk:bool=False) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query (str): 하나의 Query를 받습니다.\n",
    "        k (Optional[int] = 1): 상위 몇 개(Top-k)의 Passage를 반환할지 정합니다.\n",
    "        bulk (bool = False): 여러개의 문서를 반환할지 제어합니다.\n",
    "    Note:\n",
    "        vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "    \"\"\"\n",
    "    if bulk and isinstance(query, list): # bulk(여러 passage)\n",
    "        query_vec = tfidfv.transform(query)\n",
    "\n",
    "    elif not bulk and isinstance(query, str): # not bulk(단일 passage)\n",
    "        query_vec = tfidfv.transform([query])\n",
    "\n",
    "    assert (np.sum(query_vec) != 0), \"query에 vectorizer의 vocab에 없는 단어만 존재합니다.\"\n",
    "\n",
    "    result = query_vec * p_embedding.T\n",
    "    if not isinstance(result, np.ndarray): # 만약 넘파이가 아니라면 넘파이 배열로 바꿔주기\n",
    "        result = result.toarray()\n",
    "\n",
    "    if bulk:\n",
    "        doc_score = []\n",
    "        doc_indices = []\n",
    "        for i in range(result.shape[0]): # 문서 갯수만큼 반환\n",
    "            sorted_result = np.argsort(result[i, :])[::-1]\n",
    "            doc_score.append(result[i, :][sorted_result].tolist()[:k])\n",
    "            doc_indices.append(sorted_result.tolist()[:k])\n",
    "\n",
    "    elif not bulk:\n",
    "        sorted_result = np.argsort(result.squeeze())[::-1]\n",
    "        doc_score = result.squeeze()[sorted_result].tolist()[:k]\n",
    "        doc_indices = sorted_result.tolist()[:k]\n",
    "\n",
    "    return doc_score, doc_indices\n",
    "\n",
    "def get_relevant_doc_faiss(\n",
    "    query: str,\n",
    "    tfidfv,\n",
    "    indexer,\n",
    "    k: Optional[int] = 1,\n",
    "    bulk: bool=False) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query (str):\n",
    "            하나의 Query를 받습니다.\n",
    "        k (Optional[int] = 1):\n",
    "            상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        bulk (bool = False): 여러개의 문서를 반환할지 제어합니다.\n",
    "    Note:\n",
    "        vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "    \"\"\"\n",
    "\n",
    "    if bulk and isinstance(query, list): # bulk(여러 passage)\n",
    "        query_vec = tfidfv.transform(query)\n",
    "\n",
    "    elif not bulk and isinstance(query, str): # not bulk(단일 passage)\n",
    "        query_vec = tfidfv.transform([query])\n",
    "\n",
    "    assert (np.sum(query_vec) != 0), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "    q_emb = query_vec.toarray().astype(np.float32)\n",
    "    D, I = indexer.search(q_emb, k)\n",
    "\n",
    "    if bulk:\n",
    "        return D.tolist(), I.tolist()\n",
    "\n",
    "    elif not bulk:\n",
    "        return D.tolist()[0], I.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def get_sparse_embedding(data_path, tfidfv, contexts):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        Passage Embedding을 만들고\n",
    "        TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "        만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pickle을 저장합니다.\n",
    "    pickle_name = f\"sparse_embedding.bin\"\n",
    "    tfidfv_name = f\"tfidv.bin\"\n",
    "    emd_path = os.path.join(data_path, pickle_name)\n",
    "    tfidfv_path = os.path.join(data_path, tfidfv_name)\n",
    "\n",
    "    if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path):\n",
    "        with open(emd_path, \"rb\") as file:\n",
    "            passage_embedding = pickle.load(file)\n",
    "        with open(tfidfv_path, \"rb\") as file:\n",
    "            tfidfv = pickle.load(file)\n",
    "        print(\"Embedding pickle load.\")\n",
    "    else:\n",
    "        print(\"Build passage embedding\")\n",
    "        passage_embedding = tfidfv.fit_transform(contexts)\n",
    "        print('passage_embedding.shape', passage_embedding.shape)\n",
    "        with open(emd_path, \"wb\") as file:\n",
    "            pickle.dump(passage_embedding, file)\n",
    "        with open(tfidfv_path, \"wb\") as file:\n",
    "            pickle.dump(tfidfv, file)\n",
    "        print(\"Embedding pickle saved.\")\n",
    "    \n",
    "    return passage_embedding, tfidfv\n",
    "\n",
    "# def build_faiss(data_path, passage_embedding, num_clusters=64):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Summary:\n",
    "#         속성으로 저장되어 있는 Passage Embedding을\n",
    "#         Faiss indexer에 fitting 시켜놓습니다.\n",
    "#         이렇게 저장된 indexer는 `get_relevant_doc`에서 유사도를 계산하는데 사용됩니다.\n",
    "\n",
    "#     Note:\n",
    "#         Faiss는 Build하는데 시간이 오래 걸리기 때문에,\n",
    "#         매번 새롭게 build하는 것은 비효율적입니다.\n",
    "#         그렇기 때문에 build된 index 파일을 저정하고 다음에 사용할 때 불러옵니다.\n",
    "#         다만 이 index 파일은 용량이 1.4Gb+ 이기 때문에 여러 num_clusters로 시험해보고\n",
    "#         제일 적절한 것을 제외하고 모두 삭제하는 것을 권장합니다.\n",
    "#     \"\"\"\n",
    "\n",
    "#     indexer_name = f\"faiss_clusters{num_clusters}.index\"\n",
    "#     indexer_path = os.path.join(data_path, indexer_name)\n",
    "#     if os.path.isfile(indexer_path):\n",
    "#         print(\"Load Saved Faiss Indexer.\")\n",
    "#         indexer = faiss.read_index(indexer_path)\n",
    "\n",
    "#     else:\n",
    "#         p_emb = passage_embedding.astype(np.float32).toarray()\n",
    "#         emb_dim = p_emb.shape[-1]\n",
    "\n",
    "#         num_clusters = num_clusters\n",
    "#         quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "\n",
    "#         indexer = faiss.IndexIVFScalarQuantizer(\n",
    "#             quantizer, quantizer.d, num_clusters, faiss.METRIC_L2\n",
    "#         )\n",
    "#         indexer.train(p_emb)\n",
    "#         indexer.add(p_emb)\n",
    "#         faiss.write_index(indexer, indexer_path)\n",
    "#         print(\"Faiss Indexer Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def get_relevant_doc(\n",
    "    query: str,\n",
    "    tfidfv,\n",
    "    p_embedding,\n",
    "    k: Optional[int] = 1, \n",
    "    bulk:bool=False) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query (str): 하나의 Query를 받습니다.\n",
    "        k (Optional[int] = 1): 상위 몇 개(Top-k)의 Passage를 반환할지 정합니다.\n",
    "        bulk (bool = False): 여러개의 문서를 반환할지 제어합니다.\n",
    "    Note:\n",
    "        vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "    \"\"\"\n",
    "    if bulk and isinstance(query, list): # bulk(여러 passage)\n",
    "        query_vec = tfidfv.transform(query)\n",
    "\n",
    "    elif not bulk and isinstance(query, str): # not bulk(단일 passage)\n",
    "        query_vec = tfidfv.transform([query])\n",
    "\n",
    "    assert (np.sum(query_vec) != 0), \"query에 vectorizer의 vocab에 없는 단어만 존재합니다.\"\n",
    "\n",
    "    result = query_vec * p_embedding.T\n",
    "    if not isinstance(result, np.ndarray): # 만약 넘파이가 아니라면 넘파이 배열로 바꿔주기\n",
    "        result = result.toarray()\n",
    "\n",
    "    if bulk:\n",
    "        doc_score = []\n",
    "        doc_indices = []\n",
    "        for i in range(result.shape[0]): # 문서 갯수만큼 반환\n",
    "            sorted_result = np.argsort(result[i, :])[::-1]\n",
    "            doc_score.append(result[i, :][sorted_result].tolist()[:k])\n",
    "            doc_indices.append(sorted_result.tolist()[:k])\n",
    "\n",
    "    elif not bulk:\n",
    "        sorted_result = np.argsort(result.squeeze())[::-1]\n",
    "        doc_score = result.squeeze()[sorted_result].tolist()[:k]\n",
    "        doc_indices = sorted_result.tolist()[:k]\n",
    "\n",
    "    return doc_score, doc_indices\n",
    "\n",
    "def get_relevant_doc_faiss(\n",
    "    query: str,\n",
    "    tfidfv,\n",
    "    indexer,\n",
    "    k: Optional[int] = 1,\n",
    "    bulk: bool=False) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query (str):\n",
    "            하나의 Query를 받습니다.\n",
    "        k (Optional[int] = 1):\n",
    "            상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        bulk (bool = False): 여러개의 문서를 반환할지 제어합니다.\n",
    "    Note:\n",
    "        vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "    \"\"\"\n",
    "\n",
    "    if bulk and isinstance(query, list): # bulk(여러 passage)\n",
    "        query_vec = tfidfv.transform(query)\n",
    "\n",
    "    elif not bulk and isinstance(query, str): # not bulk(단일 passage)\n",
    "        query_vec = tfidfv.transform([query])\n",
    "\n",
    "    assert (np.sum(query_vec) != 0), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "    q_emb = query_vec.toarray().astype(np.float32)\n",
    "    D, I = indexer.search(q_emb, k)\n",
    "\n",
    "    if bulk:\n",
    "        return D.tolist(), I.tolist()\n",
    "\n",
    "    elif not bulk:\n",
    "        return D.tolist()[0], I.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 717, 2259, 1127, 2069, 1059, 2069, 575, 28674, 18, 3]\n",
      "['나', '##는', '밥', '##을', '먹', '##을', '것', '##이다', '.']\n",
      "[2, 717, 2259, 5538, 1127, 2069, 1059, 2359, 2062, 18, 3]\n",
      "['나', '##는', '어제', '밥', '##을', '먹', '##었', '##다', '.']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"나는 밥을 먹을 것이다.\"\n",
    "text2 = \"나는 어제 밥을 먹었다.\"\n",
    "print(tokenizer.encode(text1))\n",
    "print(tokenizer.tokenize(text1))\n",
    "print(tokenizer.encode(text2))\n",
    "print(tokenizer.tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval:\n",
    "    retrieval_path: data/\n",
    "    retrieval_data: wikipedia_documents.json\n",
    "    retrieval_class: SparseRetrieval\n",
    "    is_faiss: False\n",
    "    topk: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n",
      "Embedding pickle load.\n"
     ]
    }
   ],
   "source": [
    "import retrieval as Retrieval\n",
    "\n",
    "\n",
    "retrieval = getattr(Retrieval, 'SparseRetrieval')(\n",
    "        tokenizer = tokenizer,\n",
    "        data_path='data/',\n",
    "        context_path = 'wikipedia_documents.json',\n",
    "        is_faiss = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<retrieval.sparseretrieval.SparseRetrieval at 0x7fd68cb5da60>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
